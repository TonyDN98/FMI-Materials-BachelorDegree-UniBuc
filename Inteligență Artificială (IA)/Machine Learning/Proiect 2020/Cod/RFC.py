# -*- coding: utf-8 -*-
"""Proiect_Final.ipynb

Automatically generated by Colaboratory.

> # **Overview**

The purpose of this project (created using Google Colab) is to classify encrypted tweets by dialect into either Moldovian or Romanian language. This is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions. We will try different aproaches in order to observ which one is better for our dataset.

> # **Step 1.1: Understanding our Dataset**

For our AI, we will be using:
1.   train_samples.txt - the training data samples (contains the IDs and the encrypted tweets);
2.   train_labels.txt - the training labels (contains the IDs and the values 0, assigned for Moldavian, or 1, for Romanian);
3. validation_samples.txt - the validation data samples (contains the IDs and the encrypted tweets);
4. validation_labels.txt - the validation labels (contains the IDs and the values 0, assigned for Moldavian, or 1, for Romanian);
5. test_samples.txt - the test data samples.

As you can see, every single .txt file contains 2 not named columns: ID and tweet/label.

> # **Step 1.2: Reading/Importing our Dataset**

We will write a function that will help us import our dataset into the program.
"""

# Funcție pt a citi fișierele .txt
def citeste_fila(fila):
    with open(fila, mode="r", encoding="latin-1") as f:
        aux = f.readlines()
    return aux

# Citire Propriu-zisă
train_samples = citeste_fila('train_samples.txt')
train_labels = citeste_fila('train_labels.txt')
validation_samples = citeste_fila('validation_samples.txt')
validation_labels = citeste_fila('validation_labels.txt')
test_samples = citeste_fila('test_samples.txt')


"""> # **Step 2.1: Bag of Words**

The "*Bag of Words*" concept is a term used to specify the problems that have a 'bag of words' or a collection of text data that needs to be worked with.  The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter.

Using this process, we can convert a collection of documents to a matrix, with each document being a row and each word (token) being the column, and the corresponding (row, column) values being the frequency of occurrence of each word or token in that document.

To handle this, we will be using sklearns "*countVectorizer*" method which does the following:
*   It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.
*   It counts the occurrence of each of those tokens.

**IMPORTANT**:
*   The "*countVectorizer*" method automatically converts all tokenized words to their lower case. Our tweets are encrypted, so, for the purpose of this project, we need to set the "*lowercase*" parameter to "*False*", as capitalize letters and normal ones represent different things.
*   It also ignores all punctuation so that words followed by a punctuation mark (for example: "hello!") are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: "hello"). For our project, this is an useful feature, as our words from the encrypted tweets contain alphanumeric characters.
*   The third parameter to take note of is the stop_words parameter. Stop words refer to the most commonly used words in a language. For our dataset, this feature is not appropriate so we will set it to "*None*".

> # **Step 2.2:  Implementing Bag of Words in scikit-learn**
"""

# CountVectorizer pt BoW
from sklearn.feature_extraction.text import CountVectorizer
# Tweets codate -> nu e nevoie de lowercase sau stop_words
CV = CountVectorizer(max_features = 100000, lowercase = False, stop_words = None)

"""As we recall, every single .txt file contains 2 not named columns: ID and tweet/label; so we need to separate them in order to use the "*countVectorizer*" feature. As we will need the labels later on, we will do it here."""

# Extragere labels (0/1) și Tweets pt countVectorizer
train_labels = [train_labels[x].split('\t')[1].replace("\n", '') for x in range(len(train_labels))]
train_sentences = [train_samples[x].split('\t')[1] for x in range(len(train_samples))]

print(train_labels[0:10])
print(train_sentences[0:10])

# Aplicăm countVectorizer pe Datele noastre
'''
fit_transform = fit + trasnform   AKA
Învățăm vocabularul dicționarului și o transformăm în matrice (cuvinte-val)
'''
sentences_CV = CV.fit_transform(train_sentences).toarray()

"""> # **Step 3.1:  Training / Testing**

Now we can proceed with our project. We already have our dataset split into train, validation and test; so we don't need to interfere.

We have to edit the test data so we can process it (we will separate the IDs from the tweets).
"""

# Preprocesarea Datelor de Test

# Extragere ID-uri din Setul de date pentru Test
test_IDs = [test_samples[x].split('\t')[0] for x in range(len(test_samples))]
print(test_IDs[0:10])

# Extragere Tweets din Setul de date pentru Test
test_sentences = [test_samples[x].split('\t')[1] for x in range(len(test_samples))]
print(test_sentences[0:10])

# Transformăm setul de date pt Test și întoarcem matricea. (!!! NU facem fit)
test_CV = CV.transform(test_sentences).toarray()

"""> # **Step 3.2:  Choosing the Best Algorithm**

All is left for us to do is to determine which algorithm has the best accuracy for our dataset.
"""

###### Forest
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=20000)
rfc.fit(sentences_CV, train_labels)

"""Now that our algorithm has been trained using the training data set we can now make some predictions on the test data. And we will save our predictions in a .csv file."""


### FOREST
prediction = rfc.predict(test_CV)



# Salvare Predicții în fișier .csv separat
import pandas as pd
rezultat = pd.DataFrame(data={"id":test_IDs, "label": prediction} )
rezultat.to_csv("Clasificator.csv", index = False)

"""Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. For this, we will train our model on our validation set and we will compare the prediction with the actual labels. So we repeat the steps above, but for the validation dataset."""


# Extragere labels (0/1) și Tweets pt countVectorizer
validation_labels = [validation_labels[x].split('\t')[1].replace("\n", '') for x in range(len(validation_labels))]
validation_sentences = [validation_samples[x].split('\t')[1] for x in range(len(validation_samples))]


# Transformăm setul de date pt Test și întoarcem matricea
validation_CV = CV.transform(validation_sentences).toarray()


# Facem prezicerile pentru setul de date pt Test
prediction_validation = rfc.predict(validation_CV)

# Pentru a verifica acuratețea și f1-score
from sklearn.metrics import accuracy_score, f1_score
print('Accuracy score: ', accuracy_score(validation_labels, prediction_validation))
print('F1 score: ', f1_score(validation_labels, prediction_validation, average='macro'))

# Matricea de Confuzie și Raport
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(validation_labels, prediction_validation)
r = classification_report(validation_labels, prediction_validation)

# Afișare Matrice Confuzie
print('Confusion Matrix :\n')
print(cm)

# Afișare Raport
print('\n\nReport :')
print(r)